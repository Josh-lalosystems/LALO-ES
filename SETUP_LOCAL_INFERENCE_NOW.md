# Copyright (c) 2025 LALO AI SYSTEMS, LLC. All rights reserved.
#
# PROPRIETARY AND CONFIDENTIAL
#
# This file is part of LALO AI Platform and is protected by copyright law.
# Unauthorized copying, modification, distribution, or use of this software,
# via any medium, is strictly prohibited without the express written permission
# of LALO AI SYSTEMS, LLC.
#

# Setup Local Inference - Start in 30 Minutes

## Quick Start: Get LALO Running Locally TODAY

This guide gets you running with **true local AI** in under an hour.

## Step 1: Check Your Hardware (2 minutes)

```bash
# Check GPU
nvidia-smi

# Check VRAM
python -c "import logging, torch; logging.basicConfig(level=logging.INFO); logging.getLogger('lalo.docs').info(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB')"
```

**Minimum Requirements:**
- ðŸŸ¢ 16GB+ VRAM â†’ Can run DeepSeek-V3 quantized + Liquid
- ðŸŸ¡ 8-15GB VRAM â†’ Can run Liquid models only (still very capable!)
- ðŸ”´ <8GB VRAM â†’ CPU inference (slower but works)

## Step 2: Install Inference Backend (10 minutes)

### Option A: Ollama (Easiest - Recommended for MVP)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull models
ollama pull deepseek-r1:7b           # 7B reasoning model (4GB)
ollama pull qwen2.5:3b               # Fast 3B model (2GB)
ollama pull mistral:7b               # Solid 7B model (4GB)

# Test it
ollama run mistral "Hello, explain what you are"
```

### Option B: vLLM (Production - Best Performance)

```bash
# Install vLLM
pip install vllm

# Install transformers
pip install transformers accelerate
```

## Step 3: Download Models (15 minutes)

### Liquid Nanos (Small & Fast - Always install these)

```python
# File: download_models.py
from huggingface_hub import snapshot_download
import os

os.makedirs("./models", exist_ok=True)

# Liquid Tool (1.2B) - For function calling
print("Downloading Liquid Tool 1.2B...")
snapshot_download(
    repo_id="liquid/LiquidAI-1.2B-Tool-GGUF",
    local_dir="./models/liquid-tool-1.2b",
    allow_patterns=["*.gguf"]
)

# Liquid Extract (350M) - For data extraction
print("Downloading Liquid Extract 350M...")
snapshot_download(
    repo_id="liquid/LiquidAI-350M-Extract-GGUF",
    local_dir="./models/liquid-extract-350m",
    allow_patterns=["*.gguf"]
)

print("âœ“ Models downloaded!")
```

Run it:
```bash
python download_models.py
```

### DeepSeek (Optional - If you have 16GB+ VRAM)

```bash
# Use Ollama (easiest)
ollama pull deepseek-r1:7b

# OR download from HuggingFace
python -c "
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
    local_dir='./models/deepseek-7b'
)
"
```

## Step 4: Create Local Inference Service (5 minutes)

```python
# File: core/services/ollama_service.py

import requests
import json
from typing import Dict, Any, AsyncGenerator
import asyncio

class OllamaService:
    """Simple Ollama integration for local inference"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url

    async def generate(
        self,
        prompt: str,
        model: str = "mistral:7b",
        stream: bool = False,
        **kwargs
    ) -> str:
        """Generate response from local model"""

        url = f"{self.base_url}/api/generate"
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": kwargs.get("temperature", 0.7),
                "top_p": kwargs.get("top_p", 0.9),
                "max_tokens": kwargs.get("max_tokens", 1024),
            }
        }

        if stream:
            return self._stream_response(url, payload)
        else:
            response = requests.post(url, json=payload)
            response.raise_for_status()
            return response.json()["response"]

    async def _stream_response(self, url: str, payload: Dict):
        """Stream response chunks"""
        async def stream():
            response = requests.post(url, json=payload, stream=True)
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    if "response" in chunk:
                        yield chunk["response"]

        return stream()

    def list_models(self) -> list:
        """List available local models"""
        response = requests.get(f"{self.base_url}/api/tags")
        return [m["name"] for m in response.json()["models"]]


# Global instance
ollama_service = OllamaService()
```

## Step 5: Integrate with AIService (5 minutes)

```python
# File: core/services/ai_service.py (modify existing)

from core.services.ollama_service import ollama_service

class LocalAIModel(BaseAIModel):
    """Wrapper for local Ollama model"""

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.ollama = ollama_service

    async def generate(self, prompt: str, **kwargs) -> str:
        return await self.ollama.generate(
            prompt,
            model=self.model_name,
            **kwargs
        )

    async def stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        stream = await self.ollama.generate(
            prompt,
            model=self.model_name,
            stream=True,
            **kwargs
        )
        async for chunk in stream:
            yield chunk


class AIService:
    def __init__(self, database_service=None):
        self.models: Dict[str, Dict[str, BaseAIModel]] = {}
        self.db = database_service

    def initialize_local_models(self):
        """Initialize local models (no API keys needed!)"""

        # Get available Ollama models
        try:
            available = ollama_service.list_models()
        except:
            available = []

        self.models["local_user"] = {}

        # Add available models
        for model_name in available:
            self.models["local_user"][model_name] = LocalAIModel(model_name)

        # Fallback defaults
        if not available:
            print("âš ï¸  No Ollama models found. Install with: ollama pull mistral:7b")

        return list(self.models["local_user"].keys())

# Initialize on startup
ai_service = AIService()
ai_service.initialize_local_models()
```

## Step 6: Update Frontend (2 minutes)

```typescript
// File: lalo-frontend/src/components/user/UnifiedLALO.tsx

// Change default model selector
const [model, setModel] = useState('mistral:7b'); // Was 'gpt-3.5-turbo'

// Update available models
const availableModels = [
  { value: 'mistral:7b', label: 'Mistral 7B (Local)' },
  { value: 'qwen2.5:3b', label: 'Qwen 2.5 3B (Fast)' },
  { value: 'deepseek-r1:7b', label: 'DeepSeek R1 7B (Reasoning)' },
];
```

## Step 7: Remove API Key Requirements (3 minutes)

```python
# File: core/routes/ai_routes.py

@router.post("/api/ai/chat")
async def send_ai_request(
    request: AIRequest,
    current_user: str = Depends(get_current_user)
) -> AIResponse:
    """Send AI request - NO API KEYS NEEDED!"""

    try:
        # Use local models - no key check needed!
        if current_user not in ai_service.models:
            # Initialize local models
            ai_service.initialize_local_models()

        # Use "local_user" instead of actual user
        user_key = "local_user"

        available = ai_service.get_available_models(user_key)
        model = request.model or (available[0] if available else "mistral:7b")

        if not available:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="No local models available. Install with: ollama pull mistral:7b"
            )

        # Generate using local model
        generated = await ai_service.generate(
            request.prompt,
            model_name=model,
            user_id=user_key,
            max_tokens=request.max_tokens or 1024,
            temperature=request.temperature or 0.7,
        )

        # Return response
        return AIResponse(
            id=str(uuid4()),
            response=generated,
            model=model,
            usage={
                "prompt_tokens": 0,  # Ollama doesn't report these
                "completion_tokens": 0,
                "total_tokens": 0
            },
            created_at=datetime.now().isoformat(),
        )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Local inference failed: {str(e)}"
        )
```

## Step 8: Test It! (2 minutes)

```bash
# Start Ollama (if not running)
ollama serve

# In another terminal, start LALO backend
python app.py

# Open browser
# http://localhost:8000/lalo
```

**Try it:**
1. Select "Mistral 7B (Local)" from dropdown
2. Type: "Explain quantum computing in simple terms"
3. Press Enter
4. **NO API KEYS NEEDED!** âœ¨

## Verification Checklist

- [ ] Ollama installed and running (`ollama list` shows models)
- [ ] At least one model downloaded (mistral, qwen, deepseek)
- [ ] `ollama_service.py` created
- [ ] `ai_service.py` updated to use local models
- [ ] Frontend updated with local model names
- [ ] No API key errors when making requests
- [ ] Responses generated locally

## Performance Expectations

### Mistral 7B (4GB VRAM)
- Speed: ~20-50 tokens/sec (RTX 4090)
- Quality: Very good, comparable to GPT-3.5
- Best for: General chat, coding, reasoning

### Qwen 2.5 3B (2GB VRAM)
- Speed: ~50-100 tokens/sec
- Quality: Good for size
- Best for: Fast responses, simple tasks

### DeepSeek R1 7B (4GB VRAM)
- Speed: ~20-40 tokens/sec
- Quality: Excellent reasoning
- Best for: Math, logic, complex tasks

## Cost Analysis

### Before (Cloud):
```
OpenAI GPT-4: $0.03 per 1K tokens
1M tokens/month = $30,000/month
```

### After (Local):
```
Hardware: $2,000 (one-time RTX 4090)
Power: ~$50/month
Total: $2,050 first month, $50/month after

ROI: Break-even in 3 days! ðŸŽ‰
```

## Troubleshooting

### Ollama not found
```bash
# Restart Ollama service
sudo systemctl restart ollama

# Or run manually
ollama serve
```

### Model not responding
```bash
# Check if model loaded
ollama list

# Re-download if needed
ollama pull mistral:7b
```

### Out of memory
```bash
# Use smaller model
ollama pull qwen2.5:3b

# Or use 4-bit quantization
ollama pull mistral:7b-q4_0
```

### Slow generation
- Use smaller model (qwen 3B instead of mistral 7B)
- Reduce `max_tokens`
- Check GPU utilization: `nvidia-smi`

## What This Gets You

âœ… **Zero API costs** - Run unlimited queries
âœ… **Full privacy** - Data never leaves your machine
âœ… **Offline operation** - Works without internet
âœ… **Fast responses** - 20-100 tokens/second
âœ… **No vendor lock-in** - Own the entire stack
âœ… **Customizable** - Fine-tune on your data
âœ… **Compliant** - Meets all regulatory requirements

## Next Steps

Once this works:
1. âœ… Test with different models
2. âœ… Benchmark performance on your hardware
3. âœ… Fine-tune models on your data
4. âœ… Add more specialized models (coding, math, etc.)
5. âœ… Deploy to production

## Summary

You now have:
- âœ… Local AI inference (no cloud!)
- âœ… No API keys required
- âœ… Multiple models available
- âœ… Cost: ~$50/month vs $30,000/month
- âœ… Complete ownership and control

**This is the future of LALO - a true on-premise AI platform!** ðŸš€
