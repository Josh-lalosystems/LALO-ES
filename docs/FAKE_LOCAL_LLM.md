# Fake Local LLM for Testing

## Overview

The `FakeLocalInferenceServer` is a test-friendly implementation of the local LLM interface that provides deterministic, fast responses for CI testing without requiring native llama-cpp binaries or model files.

## Features

- ✅ **Always Available**: No need to install llama-cpp-python or download model files
- ✅ **Deterministic**: Same input always produces same output for reliable testing
- ✅ **Fast**: Responses are generated instantly without actual model inference
- ✅ **Compatible**: Implements the same interface as `LocalInferenceServer`
- ✅ **JSON Responses**: Returns properly formatted JSON for routing and confidence models

## Usage

### Enabling Fake LLM

Set the `USE_FAKE_LOCAL_LLM` environment variable to `1`:

```bash
export USE_FAKE_LOCAL_LLM=1
python -m pytest
```

Or inline:

```bash
USE_FAKE_LOCAL_LLM=1 python -m pytest
```

### In CI

The `.github/workflows/llama-integration.yml` workflow automatically enables the fake LLM:

```yaml
- name: Run tests with fake local LLM
  run: |
    python -m pytest tests/ -v --tb=short
  env:
    USE_FAKE_LOCAL_LLM: "1"
```

### In Tests

Tests can check if fake LLM is enabled:

```python
import os
import pytest

pytestmark = pytest.mark.skipif(
    os.getenv("USE_FAKE_LOCAL_LLM") != "1",
    reason="Fake LLM tests only run when USE_FAKE_LOCAL_LLM=1"
)

def test_with_fake_llm():
    from core.services.local_llm_service import local_llm_service
    assert local_llm_service.is_available()
```

## Behavior

### Model-Specific Responses

The fake LLM returns different types of responses based on the model:

#### liquid-tool (Router Model)
Always returns routing decision JSON:

```json
{
  "path": "simple" or "complex",
  "complexity": 0.0-1.0,
  "confidence": 0.85,
  "reasoning": "Automated routing decision from fake LLM",
  "recommended_model": "tinyllama",
  "requires_tools": true/false,
  "requires_workflow": true/false
}
```

Complexity is determined by keywords in the prompt:
- High complexity (0.8): "design", "analyze", "optimize", "architecture"
- Medium complexity (0.5): "explain", "research", "create"
- Low complexity (0.3): other prompts

#### qwen-0.5b (Confidence Model)
Always returns confidence scoring JSON:

```json
{
  "confidence": 0.0-1.0,
  "scores": {
    "factual": 0.0-1.0,
    "consistent": 0.0-1.0,
    "complete": 0.0-1.0,
    "grounded": 0.0-1.0
  },
  "issues": [],
  "recommendation": "accept" | "retry" | "escalate" | "human_review",
  "reasoning": "Automated confidence scoring from fake LLM"
}
```

Confidence is determined by keywords in the prompt:
- High (0.85): "comprehensive", "detailed"
- Low (0.3): "hmm", "i don't know"
- Medium (0.7): other prompts

#### tinyllama (General Chat)
Returns natural language responses:

```
"This is a response generated by the fake local LLM for testing purposes. 
It simulates realistic output without requiring actual model inference."
```

### Streaming

The fake LLM supports streaming via `generate_stream()`:
- Splits the response into words
- Yields each word as a separate chunk
- Simulates streaming delay with tiny sleep intervals

## Real vs Fake Server

| Feature | Real Server | Fake Server |
|---------|-------------|-------------|
| Dependencies | llama-cpp-python | None |
| Model Files | Required | Not needed |
| Speed | Varies (seconds) | Instant (<0.1s) |
| Responses | Non-deterministic | Deterministic |
| GPU Support | Yes | N/A |
| Memory Usage | High (GBs) | Minimal |
| Use Case | Production, Dev | Testing, CI |

## Switching Between Real and Fake

The service automatically switches based on the environment variable:

```python
# Default: use real server
from core.services.local_llm_service import local_llm_service
# -> LocalInferenceServer (requires llama-cpp-python)

# With USE_FAKE_LOCAL_LLM=1
from core.services.local_llm_service import local_llm_service
# -> FakeLocalInferenceServer (always available)
```

## Testing

Run the fake LLM test suite:

```bash
USE_FAKE_LOCAL_LLM=1 python -m pytest tests/test_fake_local_llm.py -v
```

Expected output:
```
tests/test_fake_local_llm.py::TestFakeLocalInferenceServer::test_service_type PASSED
tests/test_fake_local_llm.py::TestFakeLocalInferenceServer::test_is_available PASSED
tests/test_fake_local_llm.py::TestFakeLocalInferenceServer::test_model_loading PASSED
tests/test_fake_local_llm.py::TestFakeLocalInferenceServer::test_routing_decision PASSED
tests/test_fake_local_llm.py::TestFakeLocalInferenceServer::test_confidence_scoring PASSED
...
============ 12 passed in 0.84s ============
```

## Implementation Details

### Class Structure

```python
class FakeLocalInferenceServer:
    def __init__(self, model_dir: str = "./models")
    def is_available(self) -> bool
    def load_model(self, model_name: str) -> bool
    def unload_model(self, model_name: str)
    def unload_all_models(self)
    async def generate(self, prompt: str, model_name: str, ...) -> str
    async def generate_stream(self, prompt: str, model_name: str, ...)
    def get_available_models(self) -> List[Dict[str, Any]]
    def get_loaded_models(self) -> List[str]
    def shutdown(self)
```

### Design Decisions

1. **Always Available**: Returns `True` from `is_available()` to simplify testing
2. **Model Loading**: Simulates loading but doesn't actually load anything
3. **JSON Responses**: Model-specific responses return properly formatted JSON
4. **Deterministic**: Same input always produces same output for reliable tests
5. **Fast**: Minimal processing to keep tests fast

## Troubleshooting

### Fake LLM not being used

Check that the environment variable is set:

```bash
echo $USE_FAKE_LOCAL_LLM  # Should print "1"
```

### Tests expecting real responses

Some tests may expect specific responses from real models. Update them to:
1. Skip when fake LLM is active
2. Adjust assertions to work with deterministic responses
3. Use the `test_fake_local_llm.py` pattern

### JSON parsing errors

Ensure you're using the correct model name:
- `liquid-tool` for routing decisions
- `qwen-0.5b` for confidence scoring
- `tinyllama` for general text

## Future Enhancements

Potential improvements:
- [ ] More sophisticated keyword detection for complexity
- [ ] Support for custom response templates
- [ ] Simulate errors and edge cases
- [ ] Configurable response delays for performance testing
- [ ] Support for conversation history

## See Also

- [Local Inference Strategy](../LOCAL_INFERENCE_STRATEGY.md)
- [GitHub Actions Workflows](../.github/workflows/)
- [Test Suite](../tests/test_fake_local_llm.py)
